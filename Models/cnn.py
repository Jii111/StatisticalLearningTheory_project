# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11WQgTdw97K94rN6LEEU4U5dIoG8Thfuq
"""

import json
import os
from pycocotools.coco import COCO

# 본인 경로로 수정
train_dataset_dir = "/content/drive/MyDrive/train_augmentation"
train_json_dir = "/content/drive/MyDrive/train_augmentation/_annotations.coco.json"

test_dataset_dir = "/content/drive/MyDrive/test"
test_json_dir = "/content/drive/MyDrive/test/_annotations.coco.json"

valid_dataset_dir = "/content/drive/MyDrive/valid"
valid_json_dir = "/content/drive/MyDrive/valid/_annotations.coco.json"

# json 파일을 읽고 key별로 정보 저장
with open(train_json_dir, 'r') as f:
    train_json = json.loads(f.read())

# Extract information from the training dataset JSON
train_images = train_json['images']
train_categories = train_json['categories']
train_annotations = train_json['annotations']

# categories_names: category_id와 이름을 매핑해주는 변수
categories_names = ['None']
for cat_dict in train_categories:
    categories_names.append(cat_dict['name'])

# coco: train_json 파일을 COCO api 형태로 불러온 변수
coco_train = COCO(train_json_dir)

# test
with open(test_json_dir, 'r') as f:
    test_json = json.loads(f.read())

# Extract information from the test dataset JSON
test_images = test_json['images']
test_categories = test_json['categories']
test_annotations = test_json['annotations']

coco_test = COCO(test_json_dir)

# valid
with open(valid_json_dir, 'r') as f:
    valid_json = json.loads(f.read())


# Extract information from the validation dataset JSON
valid_images = valid_json['images']
valid_categories = valid_json['categories']
valid_annotations = valid_json['annotations']

coco_valid = COCO(valid_json_dir)

!pip install torchvision

import torch
import torch.nn as nn # 신경망들이 포함됨
import torch.optim as optim # 최적화 알고리즘들이 포함됨
import torch.nn.init as init # 텐서에 초기값을 줌

import torchvision.datasets as datasets # 이미지 데이터셋 집합체
import torchvision.transforms as transforms # 이미지 변환 툴

from torch.utils.data import DataLoader # 학습 및 배치로 모델에 넣어주기 위한 툴

import numpy as np
import matplotlib.pyplot as plt

batch_size = 100
learning_rate = 0.0002
num_epoch = 10

mnist_train = datasets.MNIST(root="/content/drive/MyDrive/train_augmentation", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)
mnist_valid = datasets.MNIST(root="/content/drive/MyDrive/valid", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)

train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)
valid_loader = DataLoader(mnist_valid, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)

class CNN(nn.Module):
    def __init__(self):
    	# super함수는 CNN class의 부모 class인 nn.Module을 초기화
        super(CNN, self).__init__()

        # batch_size = 100
        self.layer = nn.Sequential(
            # [100,1,28,28] -> [100,16,24,24]
            nn.Conv2d(in_channels=1,out_channels=16,kernel_size=5),
            nn.ReLU(),

            # [100,16,24,24] -> [100,32,20,20]
            nn.Conv2d(in_channels=16,out_channels=32,kernel_size=5),
            nn.ReLU(),

            # [100,32,20,20] -> [100,32,10,10]
            nn.MaxPool2d(kernel_size=2,stride=2),

            # [100,32,10,10] -> [100,64,6,6]
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
            nn.ReLU(),

            # [100,64,6,6] -> [100,64,3,3]
            nn.MaxPool2d(kernel_size=2,stride=2)
        )
        self.fc_layer = nn.Sequential(
        	# [100,64*3*3] -> [100,100]
            nn.Linear(64*3*3,100),
            nn.ReLU(),
            # [100,100] -> [100,10]
            nn.Linear(100,10)
        )

    def forward(self,x):
    	# self.layer에 정의한 연산 수행
        out = self.layer(x)
        # view 함수를 이용해 텐서의 형태를 [100,나머지]로 변환
        out = out.view(batch_size,-1)
        # self.fc_layer 정의한 연산 수행
        out = self.fc_layer(out)
        return out

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model = CNN().to(device)

loss_func = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

#train data
loss_arr =[]
for i in range(num_epoch):
    for j,[image,label] in enumerate(train_loader):
        x = image.to(device)
        y= label.to(device)

        optimizer.zero_grad()

        output = model.forward(x)

        loss = loss_func(output,y)
        loss.backward()
        optimizer.step()

        if j % 1000 == 0:
            print(loss)
            loss_arr.append(loss.cpu().detach().numpy())

# 모델 저장
torch.save(model, 'your_model.pth')

# 저장된 모델 불러오기
model = torch.load('your_model.pth')

# 모델을 evaluation mode로 설정
model.eval()

correct = 0
total = 0

# evaluate model
model.eval()

with torch.no_grad():
    for image,label in valid_loader:
        x = image.to(device)
        y= label.to(device)

        output = model.forward(x)

        # torch.max함수는 (최댓값,index)를 반환
        _,output_index = torch.max(output,1)

        # 전체 개수 += 라벨의 개수
        total += label.size(0)

        # 도출한 모델의 index와 라벨이 일치하면 correct에 개수 추가
        correct += (output_index == y).sum().float()

    # 정확도 도출
    print("Accuracy of Test Data: {}%".format(100*correct/total))